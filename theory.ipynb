{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 : 2.6\n",
    "\n",
    "We initialize the expected values optimistically, i.e., at a value much higher than the expected mean. This results in the algorithm always (with very high probability) getting a reward lower than the expected value. After K such failures (10 in our case), allowing for a few steps because of the probability distributions, the maximum of expected values will the the true maximum (optimal choice), because this has decreased least among all the K expected rewards (the same initialization - constant*(reward - same initialization) for K bandits) as the reward on average has been the maximum. \n",
    "\n",
    "Now that it has found the true maximum, due to the small step sizes, it will stay at the optimal choice for a few steps until it reaches the next largest (sub-optimal) choice. This results in the spike in accuracy we see. \n",
    "\n",
    "We see upwards and downwards spikes in the graph trend. This is because once the algorithm has brought down the optimal choice's expected rewards to below the other (sub-optimal) reward expectations, it chooses the sub-optimal choices repeatedly until again the optimal choice is the maximum in the array. This leads to a fall and then a spike, and so on and so forth. This height/depth of the spikes decrease as the graph progresses because the expected values become closer to the true rewards, and hence the instability decreases, the gap between the true optimal and sub-optimal expected rewards is realized, and the algorithm makes the optimal choice more often.\n",
    "\n",
    "We can show the validity of these claims by plotting the same graph for different values of K and different optimistic intializations (optimal selection fraction vs epochs). The value of K should determine roughly where the spikes occur, and the more drastic the optimistic initialization the more drastic the spikes. Showing below graphs for K=3 vs K=20 (spikes occur earlier) and initial = 20 vs initial=5 (exceedingly drastic spikes).\n",
    "\n",
    "<img src=\"K=20,3.png\">\n",
    "\n",
    "<img src=\"initial=20,5.png\">\n",
    "\n",
    "\n",
    "In the non-stationary case, we see that the greedy approach does not work as well as in the stationary case. This is because the drive for exploration is at the beginning of time only, once it has derived estimates for all the values, it is simply a greedy (argmax) approach. It is also a slower ascent to peak accuracy as each epoch the distributions change and hence the algorithm does not converge on the optimal choice as it did in the stationary case, once it is on the optimal case as well the distribution does not remain constant. However, the graph of the optimistic initialisation vs the epsilon greedy method can be slightly misleading if the degree of random deviation (measure of the random walk taken by our true reward mean) is small, or we do not examine the graph for enough epochs. Over time, with enough deviation from the intial random means, we see that the greedy algorithm does not learn the new means at all, while the sample average performs better. If we increase the degree of random walks (increase deviation, mean, perhaps) we see that the method soon becomes useless and sees a steady decrease in accuracy. Shown below for example is the graph (optimal selection fraction vs epochs) for a non-stationary problem with the greedy optimistic approach and with 1500 epochs instead of 1000. We can see how it fares poorly against the epsilon greedy approach.\n",
    "\n",
    "<img src=\"small random walks.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: 2.7 \n",
    "<img src=\"q3-1.jpg\">\n",
    "    \n",
    "<img src=\"q3-2.jpg\">\n",
    "    \n",
    "<img src=\"q3-3.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Compare UCB to Optimistic value and epsilon greedy\n",
    "\n",
    "\n",
    "We use UCB action selection to model the uncertainty in the estimate of an action's value. We take the max over a sort of upper bound on the true values of a, by taking into account the number of times the action has been selected as well as the amount of time gone by. Each time the action is selected, the uncertainty is reduced, while as time moves on without the selection of the action the numerator in our uncertainty term increased and the denominator remains constant, thus increasing the uncertainty. \n",
    "\n",
    "Stationary settings: In stationary settings, the average reward is dominated by optimistic choice, as once it identifies the optimal choice it almost always chooses that as there is no scope for exploration except at the beginning of time for the optimistic strategy. The UCB strategy performs next best, closely followed by the espilon greedy strategy. This is because while both UCB and epsilon greedy have a tendency to explore, the UCB exploration has lesser and lesser marginal increases because of the natural log, while for epsilon greedy the exploration term is constant. This is reflected in the graphs shown below, of (a) the average reward and (b) the fraction of optimal choice vs epochs.\n",
    "\n",
    "<img src=\"stationary_ucb_reward.png\">\n",
    "<img src=\"stationary_ucb_fraction.png\">\n",
    "\n",
    "Non-stationary setting: Here, over sufficient epochs, epsilon greedy should always perform better because it continues to explore at a constant rate as compared to UCB, which explores at an ever decreasing rate and the optimistic method, which does not explore. None of the algorithms give great performance however, as none of them really adapt to the non-stationary setting as well as the stationary setting. The more informative graph here is graph of optimal choice fraction vs epochs for the three methods, which highlight the points aboove.\n",
    "\n",
    "<img src=\"random_ucb_fraction.png\">\n",
    "<img src=\"random_ucb_reward.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
